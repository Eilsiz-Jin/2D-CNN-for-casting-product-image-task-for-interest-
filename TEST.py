import os
import copy
import torch
import torch.nn as nn
import numpy as np
import torch.nn.functional as function
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms


import kagglehub
# ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„æ•°æ®é›†
path = kagglehub.dataset_download("ravirajsinh45/real-life-industrial-dataset-of-casting-product")
print("Path to dataset files:", path)

# 2. é€‰è®¾å¤‡ï¼ˆGPU / CPUï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using {device} device")

# 3. å¤„ç†æ•°æ®é›†
data_root = path
train_dir = os.path.join(data_root, "casting_data", "casting_data", "train")
test_dir  = os.path.join(data_root, "casting_data", "casting_data", "test")

train_transform = transforms.Compose([
    transforms.Lambda(lambda img: img.convert("RGB")),  # è½¬æˆ3é€šé“ï¼Œé€‚é… in_channels=3 çš„ç½‘ç»œ
    transforms.Resize((256, 256)),                  # æˆ–è€… (128,128)/(300,300)ï¼Œçœ‹ä½ æƒ³ç”¨å¤šå¤§çš„è¾“å…¥
    transforms.ToTensor(),                        # [0,255] -> [0,1]
    transforms.RandomHorizontalFlip(p=0.4),   # å¯é€‰å¢žå¼ºï¼šæ°´å¹³ç¿»è½¬
    transforms.RandomRotation(degrees=10),    # å¯é€‰å¢žå¼ºï¼šå°è§’åº¦æ—‹è½¬
    transforms.Normalize(mean=[0.5, 0.5, 0.5],
                         std=[0.5, 0.5, 0.5])
])

test_transform = transforms.Compose([
    transforms.Lambda(lambda img: img.convert("RGB")),  # è½¬æˆ3é€šé“ï¼Œé€‚é… in_channels=3 çš„ç½‘ç»œ
    transforms.Resize((256, 256)),                  # æˆ–è€… (128,128)/(300,300)ï¼Œçœ‹ä½ æƒ³ç”¨å¤šå¤§çš„è¾“å…¥
    transforms.ToTensor(),                        # [0,255] -> [0,1]
    transforms.Normalize(mean=[0.5, 0.5, 0.5],
                         std=[0.5, 0.5, 0.5])
])

training_data = datasets.ImageFolder(
    root=train_dir,
    transform=train_transform
)

test_data = datasets.ImageFolder(
    root=test_dir,
    transform=test_transform
)
#print("classes:", training_data.classes)  # ['def_front', 'ok_front'] ä¹‹ç±»

batch_size = 32

train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)
test_dataloader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)

for X, y in train_dataloader:
    print("X shape:", X.shape)   # [32, 3, 256, 256]
    print("y shape:", y.shape)   # [32]
    break

class my2D_CNN(nn.Module):
    def __init__(self, num_classes = 2):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16,kernel_size=3, padding=1)
        self.bn1   = nn.BatchNorm2d(16)

        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.bn2   = nn.BatchNorm2d(32)

        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn3   = nn.BatchNorm2d(64)

        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn4   = nn.BatchNorm2d(128)

        # ç©ºé—´ä¸‹é‡‡æ ·ï¼šä¸¤æ¬¡ MaxPool2d(2,2)
        self.pool = nn.MaxPool2d(2, 2)
        # å°ºå¯¸å˜åŒ–ï¼š
        # 256x256 --pool--> 128x128 --pool--> 64x64

        # è¿™é‡Œä¸è¦ç›´æŽ¥ GAP åˆ° 1x1ï¼Œè€Œæ˜¯ä¿ç•™ä¸€ç‚¹ç©ºé—´æ ¼å­
        # æ¯ä¸ªæ ¼å­å¯¹åº”ä¸€å¤§å—åŒºåŸŸï¼Œæ¯”å¦‚ 64x64 -> 8x8
        self.spp = nn.AdaptiveAvgPool2d((8, 8))   # ä¹Ÿå¯ä»¥æ”¹æˆ 4x4 / æ”¹æˆ MaxPool çœ‹æ•ˆæžœ

        # 128 * 8 * 8 = 4096 ç»´
        self.fc1 = nn.Linear(128 * 8 * 8, 256)
        self.fc2 = nn.Linear(256, num_classes)

        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # [B, 1, 256, 256]
        x = self.pool(function.relu(self.bn1(self.conv1(x))))  # -> [B, 16, 128, 128]
        x = self.pool(function.relu(self.bn2(self.conv2(x))))  # -> [B, 32,  64,  64]
        x = function.relu(self.bn3(self.conv3(x)))             # -> [B, 64,  64,  64]
        x = function.relu(self.bn4(self.conv4(x)))             # -> [B, 128,  64,  64]

        # ä¿ç•™ 8x8 çš„ç²—ç©ºé—´ç½‘æ ¼
        x = self.spp(x)                                 # -> [B, 128, 8, 8]

        x = x.view(x.size(0), -1)                       # -> [B, 8192]

        x = self.dropout(function.relu(self.fc1(x)))
        x = self.fc2(x)                                 # -> [B, 2]
        return x
    
model = my2D_CNN().to(device)
print(model)



# 4. è®­ç»ƒä¸Žè¯„ä¼°æ¨¡åž‹
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-4, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

num_epochs = 24
min_epochs = 15

target_acc = 0.995   # 99.5%
target_loss = 0.025

best_acc = 0.0
best_loss = float("inf")
best_state = None

def train(dataloader, model, loss_fn, optimizer, device):
    size = len(dataloader.dataset)
    model.train()
    
    running_loss = 0.0
    running_total = 0

    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        batch_size = X.size(0)
        running_loss += loss.item() * batch_size
        running_total += batch_size
        
        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
    
    train_loss = running_loss / running_total
    print(f"Train Loss: {train_loss:.6f}")
    return train_loss

def test(dataloader, model, loss_fn, device):
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            loss = loss_fn(pred, y)

            # æ³¨æ„è¿™é‡ŒæŒ‰æ ·æœ¬æ•°åŠ æƒï¼Œæœ€åŽå†é™¤ä»¥ total
            batch_size = X.size(0)
            test_loss += loss.item() * batch_size
            correct += (pred.argmax(1) == y).sum().item()
            total += batch_size

    avg_loss = test_loss / total
    acc = correct / total

    print(f"Test Error: \n Accuracy: {100*acc:.1f}%, Avg loss: {avg_loss:.8f}\n")
    return avg_loss, acc

for epoch in range(num_epochs):
    print(f"Epoch {epoch+1}")
    print("-------------------------------")

    # ======= è®­ç»ƒé˜¶æ®µ =======
    train_loss = train(train_dataloader, model, loss_fn, optimizer, device)

    # ======= éªŒè¯é˜¶æ®µ =======
    val_loss, val_acc = test(test_dataloader, model, loss_fn, device)
    
    if epoch+1 > 10:
        scheduler.step()
    # test() å‡½æ•°ä¿æŒä¹‹å‰é‚£æ ·ï¼šmodel.eval() + no_gradï¼Œè¿”å›ž (avg_loss, acc)

    # è®°å½•â€œæ•´ä½“æœ€ä¼˜â€æ¨¡åž‹
    if (epoch + 1) >= min_epochs:
        if (val_acc > best_acc) or (val_acc == best_acc and val_loss < best_loss):
            best_acc = val_acc
            best_loss = val_loss
            best_state = copy.deepcopy(model.state_dict())
            print(f"ðŸŒŸ New best so far (after epoch {epoch+1}): acc={best_acc:.6f}, loss={best_loss:.6f}")

    # ======= Early Stoppingï¼šåªåœ¨ min_epochs ä¹‹åŽæ‰å…è®¸è§¦å‘ =======
    if (epoch + 1) >= min_epochs and val_acc >= target_acc and val_loss <= target_loss:
        print(f"âœ… Early stopping at epoch {epoch+1}: "
              f"acc={val_acc:.4f}, loss={val_loss:.6f}")
        torch.save(model.state_dict(), "model_earlystop_995acc_0035loss.pth")
        break

#ä¿å­˜è®­ç»ƒæ¨¡åž‹

# è®­ç»ƒç»“æŸåŽï¼ˆæ— è®ºæ˜¯å¦ early stopï¼‰ï¼ŒæŠŠâ€œæ•´ä½“æœ€ä¼˜â€ä¹Ÿå­˜ä¸€ä»½
if best_state is not None:
    model.load_state_dict(best_state)
    best_path = os.path.abspath("myCNN_best.pth")
    torch.save(model.state_dict(), best_path)
    print(f"Best overall model saved to: {best_path}, acc={best_acc:.4f}, loss={best_loss:.6f}")